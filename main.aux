\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\emailauthor{ahmad.asadi@aut.ac.ir}{Ahmad Asadi\fnref {label2}}
\emailauthor{safa@aut.ac.ir}{Reza Safabakhsh\fnref {label2}}
\HyPL@Entry{0<</S/D>>}
\Newlabel{label2}{1}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Baseline Encoder-Decoder Model}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Background}{3}{subsection.2.1}}
\citation{lopez2008statistical}
\citation{cho2014learning}
\citation{cho2014learning}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The basic scheme of the encoder-decoder model}}{4}{figure.1}}
\newlabel{fig:encdec}{{1}{4}{The basic scheme of the encoder-decoder model}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Encoder-Decoder Framework's Applications}{4}{subsection.2.2}}
\citation{cho2014properties}
\citation{cho2014learning}
\@writefile{toc}{\contentsline {section}{\numberline {3}The encoder-decoder model for machine translation}{6}{section.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.0.1}Formulation}{6}{subsubsection.3.0.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces One-hot vector for each word in a sample dictionary. Sentences can also be modeled using \textit  {Bag of Words} (BoW) technique in which the presence of a word in the sentence is considered without any information about the order of words.}}{7}{figure.2}}
\newlabel{fig:onehot}{{2}{7}{One-hot vector for each word in a sample dictionary. Sentences can also be modeled using \textit {Bag of Words} (BoW) technique in which the presence of a word in the sentence is considered without any information about the order of words}{figure.2}{}}
\citation{cho2014learning}
\citation{cho2014learning}
\citation{cho2014learning}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces An illustration of the first encoder-decoder based model proposed by Cho et al. \cite  {cho2014learning}.}}{8}{figure.3}}
\newlabel{fig:encdec1}{{3}{8}{An illustration of the first encoder-decoder based model proposed by Cho et al. \cite {cho2014learning}}{figure.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.0.2}Encoders (feature extraction)}{8}{subsubsection.3.0.2}}
\newlabel{eq:hidden1}{{1}{8}{Encoders (feature extraction)}{equation.3.1}{}}
\newlabel{eq:context1}{{2}{9}{Encoders (feature extraction)}{equation.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.0.3}Decoders (language modeling)}{9}{subsubsection.3.0.3}}
\newlabel{eq:obj1}{{3}{9}{Decoders (language modeling)}{equation.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Directed Acyclic Graph of dependecies of random variables in the encoder-decoder model}}{9}{figure.4}}
\newlabel{fig:BNI}{{4}{9}{Directed Acyclic Graph of dependecies of random variables in the encoder-decoder model}{figure.4}{}}
\citation{cho2014learning}
\newlabel{eq:obj2}{{4}{10}{Decoders (language modeling)}{equation.3.4}{}}
\newlabel{eq:obj3}{{5}{10}{Decoders (language modeling)}{equation.3.5}{}}
\newlabel{eq:obj4}{{6}{10}{Decoders (language modeling)}{equation.3.6}{}}
\newlabel{eq:obj03}{{7}{10}{Decoders (language modeling)}{equation.3.7}{}}
\newlabel{eq:loglikelihood1}{{8}{10}{Decoders (language modeling)}{equation.3.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Encoder Structures}{10}{section.4}}
\citation{gehring2016convolutional}
\citation{hochreiter1997long}
\citation{sutskever2014sequence}
\citation{bahdanau2014neural}
\citation{luong2015stanford}
\citation{xu2015show}
\citation{cho2014learning}
\citation{cho2014learning}
\citation{mi2016coverage}
\citation{he2016dual}
\citation{tu2017context}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Sentence as input}{11}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Image as input}{11}{subsection.4.2}}
\citation{simonyan2014very}
\citation{krizhevsky2012imagenet}
\citation{deng2009imagenet}
\citation{karpathy2015deep}
\citation{chen2017sca}
\citation{pedersoli2017areas}
\citation{he2016deep}
\citation{lu2017knowing}
\citation{rennie2017self}
\citation{anderson2017bottom}
\citation{yao2017boosting}
\citation{szegedy2016rethinking}
\citation{zhang2017actor}
\citation{ioffe2015batch}
\citation{vinyals2015show}
\citation{liu2017improved}
\citation{vinyals2015show}
\citation{vinyals2015show}
\citation{vinyals2015show}
\newlabel{eq:imgCont}{{9}{12}{Image as input}{equation.4.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Model architecture based on encoder-decoder framework for image captioning \cite  {vinyals2015show}}}{12}{figure.5}}
\newlabel{fig:imgCpt}{{5}{12}{Model architecture based on encoder-decoder framework for image captioning \cite {vinyals2015show}}{figure.5}{}}
\citation{venugopalan2014translating}
\citation{venugopalan2014translating}
\citation{venugopalan2014translating}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Video as input}{13}{subsection.4.3}}
\newlabel{eq:vid1}{{10}{13}{Video as input}{equation.4.10}{}}
\citation{yao2017boosting}
\citation{szegedy2017inception}
\citation{yao2015describing}
\citation{yao2015describing}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces An illustration of the first encoder-decoder based model for video description generation proposed by venugopalan et al. in 2015 \cite  {venugopalan2014translating}}}{14}{figure.6}}
\newlabel{fig:vid1}{{6}{14}{An illustration of the first encoder-decoder based model for video description generation proposed by venugopalan et al. in 2015 \cite {venugopalan2014translating}}{figure.6}{}}
\citation{yao2015describing}
\citation{pan2016jointly}
\citation{pan2016jointly}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The structure of 3D-CNN proposed by Yao et al. in 2015 \cite  {yao2015describing}}}{15}{figure.7}}
\newlabel{fig:3dcnn}{{7}{15}{The structure of 3D-CNN proposed by Yao et al. in 2015 \cite {yao2015describing}}{figure.7}{}}
\citation{li2018jointly}
\citation{krishna2017dense}
\citation{krishna2017dense}
\citation{krishna2017dense}
\citation{escorcia2016daps}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces An illustration of the encoder part of the proposed model by Pan et al. \cite  {pan2016jointly}}}{16}{figure.8}}
\newlabel{fig:3dcnn2}{{8}{16}{An illustration of the encoder part of the proposed model by Pan et al. \cite {pan2016jointly}}{figure.8}{}}
\citation{krishna2017dense}
\citation{krishna2017dense}
\citation{li2018jointly}
\citation{li2018jointly}
\citation{li2018jointly}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces An illustration of the first proposed encoder model for dense video captioning by Krishna et al. \cite  {krishna2017dense}}}{17}{figure.9}}
\newlabel{fig:dense1}{{9}{17}{An illustration of the first proposed encoder model for dense video captioning by Krishna et al. \cite {krishna2017dense}}{figure.9}{}}
\citation{li2018jointly}
\citation{li2018jointly}
\citation{shen2017weakly}
\citation{duan2018weakly}
\citation{zhou2018end}
\citation{wang2018bidirectional}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces An illustration of the encoder-decoder structure proposed by Li et al. for dense video captioning \cite  {li2018jointly}}}{18}{figure.10}}
\newlabel{fig:dense2}{{10}{18}{An illustration of the encoder-decoder structure proposed by Li et al. for dense video captioning \cite {li2018jointly}}{figure.10}{}}
\citation{goodfellow2016deep}
\citation{cho2014learning}
\citation{bahdanau2014neural}
\citation{luong2015effective}
\citation{wu2016google}
\citation{johnson2017google}
\citation{luong2014addressing}
\@writefile{toc}{\contentsline {section}{\numberline {5}Decoder Structures}{19}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Long term dependencies}{19}{subsection.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}LSTMs}{19}{subsection.5.2}}
\citation{donahue2015long}
\citation{gu2018stack}
\citation{venugopalan2014translating}
\citation{pan2016hierarchical}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Stacked RNNs}{20}{subsection.5.3}}
\citation{pan2016hierarchical}
\citation{pan2016hierarchical}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Stacked structure of RNNs \cite  {pan2016hierarchical}}}{21}{figure.11}}
\newlabel{fig:stacked}{{11}{21}{Stacked structure of RNNs \cite {pan2016hierarchical}}{figure.11}{}}
\citation{yu2016video}
\citation{donahue2015long}
\citation{pan2016hierarchical}
\citation{yu2016video}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Hierarchical structure of RNNs \cite  {pan2016hierarchical}}}{22}{figure.12}}
\newlabel{fig:hrne}{{12}{22}{Hierarchical structure of RNNs \cite {pan2016hierarchical}}{figure.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Vanishing gradients in stacked decoders}{22}{subsubsection.5.3.1}}
\citation{Asadi2019stacked}
\citation{wu2016google}
\citation{wang2018video}
\citation{vedantam2015cider}
\citation{li2018jointly}
\citation{banerjee2005meteor}
\citation{wang2018video}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Reinforcement learning}{23}{subsection.5.4}}
\citation{wang2018video}
\citation{wang2018video}
\citation{wang2018video}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces An illustration of the encoder-decoder based model proposed by Wang et al. \cite  {wang2018video}. The proposed decoder is trained using reinforcement learning.}}{24}{figure.13}}
\newlabel{fig:reinf1}{{13}{24}{An illustration of the encoder-decoder based model proposed by Wang et al. \cite {wang2018video}. The proposed decoder is trained using reinforcement learning}{figure.13}{}}
\newlabel{eq:reinf1}{{11}{24}{Reinforcement learning}{equation.5.11}{}}
\newlabel{eq:reinf2}{{12}{24}{Reinforcement learning}{equation.5.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces An illustration of the unrolled decoder proposed by Wang et al. \cite  {wang2018video}.}}{25}{figure.14}}
\newlabel{fig:reinf2}{{14}{25}{An illustration of the unrolled decoder proposed by Wang et al. \cite {wang2018video}}{figure.14}{}}
\newlabel{eq:reinf3}{{13}{25}{Reinforcement learning}{equation.5.13}{}}
\newlabel{eq:reinf4}{{14}{25}{Reinforcement learning}{equation.5.14}{}}
\newlabel{eq:reinf5}{{15}{25}{Reinforcement learning}{equation.5.15}{}}
\newlabel{eq:reinf6}{{16}{26}{Reinforcement learning}{equation.5.16}{}}
\newlabel{eq:reinf7}{{17}{26}{Reinforcement learning}{equation.5.17}{}}
\newlabel{eq:drtn}{{19}{26}{Reinforcement learning}{equation.5.19}{}}
\newlabel{eq:reinf8}{{20}{26}{Reinforcement learning}{equation.5.20}{}}
\newlabel{eq:nabla}{{23}{26}{Reinforcement learning}{equation.5.23}{}}
\citation{papineni2002bleu}
\citation{bahdanau2014neural}
\citation{bahdanau2014neural}
\newlabel{eq:reinf9}{{24}{27}{Reinforcement learning}{equation.5.24}{}}
\newlabel{eq:reinf10}{{25}{27}{Reinforcement learning}{equation.5.25}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Attention Mechanism}{27}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Basic Mechanism}{27}{subsection.6.1}}
\citation{xu2015show}
\citation{xu2015show}
\newlabel{eq:attn1}{{26}{28}{Basic Mechanism}{equation.6.26}{}}
\newlabel{eq:attn2}{{27}{28}{Basic Mechanism}{equation.6.27}{}}
\newlabel{eq:attn3}{{28}{28}{Basic Mechanism}{equation.6.28}{}}
\newlabel{eq:attn4}{{29}{28}{Basic Mechanism}{equation.6.29}{}}
\citation{you2016image}
\citation{lu2017knowing}
\citation{wu2018hierarchical}
\bibdata{ref}
\bibcite{lopez2008statistical}{{1}{}{{}}{{}}}
\bibcite{cho2014learning}{{2}{}{{}}{{}}}
\bibcite{cho2014properties}{{3}{}{{}}{{}}}
\bibcite{gehring2016convolutional}{{4}{}{{}}{{}}}
\bibcite{hochreiter1997long}{{5}{}{{}}{{}}}
\bibcite{sutskever2014sequence}{{6}{}{{}}{{}}}
\bibcite{bahdanau2014neural}{{7}{}{{}}{{}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Extensions}{29}{subsection.6.2}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{29}{section.7}}
\bibcite{luong2015stanford}{{8}{}{{}}{{}}}
\bibcite{xu2015show}{{9}{}{{}}{{}}}
\bibcite{mi2016coverage}{{10}{}{{}}{{}}}
\bibcite{he2016dual}{{11}{}{{}}{{}}}
\bibcite{tu2017context}{{12}{}{{}}{{}}}
\bibcite{simonyan2014very}{{13}{}{{}}{{}}}
\bibcite{krizhevsky2012imagenet}{{14}{}{{}}{{}}}
\bibcite{deng2009imagenet}{{15}{}{{}}{{}}}
\bibcite{karpathy2015deep}{{16}{}{{}}{{}}}
\bibcite{chen2017sca}{{17}{}{{}}{{}}}
\bibcite{pedersoli2017areas}{{18}{}{{}}{{}}}
\bibcite{he2016deep}{{19}{}{{}}{{}}}
\bibcite{lu2017knowing}{{20}{}{{}}{{}}}
\bibcite{rennie2017self}{{21}{}{{}}{{}}}
\bibcite{anderson2017bottom}{{22}{}{{}}{{}}}
\bibcite{yao2017boosting}{{23}{}{{}}{{}}}
\bibcite{szegedy2016rethinking}{{24}{}{{}}{{}}}
\bibcite{zhang2017actor}{{25}{}{{}}{{}}}
\bibcite{ioffe2015batch}{{26}{}{{}}{{}}}
\bibcite{vinyals2015show}{{27}{}{{}}{{}}}
\bibcite{liu2017improved}{{28}{}{{}}{{}}}
\bibcite{venugopalan2014translating}{{29}{}{{}}{{}}}
\bibcite{szegedy2017inception}{{30}{}{{}}{{}}}
\bibcite{yao2015describing}{{31}{}{{}}{{}}}
\bibcite{pan2016jointly}{{32}{}{{}}{{}}}
\bibcite{li2018jointly}{{33}{}{{}}{{}}}
\bibcite{krishna2017dense}{{34}{}{{}}{{}}}
\bibcite{escorcia2016daps}{{35}{}{{}}{{}}}
\bibcite{shen2017weakly}{{36}{}{{}}{{}}}
\bibcite{duan2018weakly}{{37}{}{{}}{{}}}
\bibcite{wang2018bidirectional}{{38}{}{{}}{{}}}
\bibcite{goodfellow2016deep}{{39}{}{{}}{{}}}
\bibcite{luong2015effective}{{40}{}{{}}{{}}}
\bibcite{wu2016google}{{41}{}{{}}{{}}}
\bibcite{johnson2017google}{{42}{}{{}}{{}}}
\bibcite{luong2014addressing}{{43}{}{{}}{{}}}
\bibcite{donahue2015long}{{44}{}{{}}{{}}}
\bibcite{gu2018stack}{{45}{}{{}}{{}}}
\bibcite{pan2016hierarchical}{{46}{}{{}}{{}}}
\bibcite{yu2016video}{{47}{}{{}}{{}}}
\bibcite{Asadi2019stacked}{{48}{}{{}}{{}}}
\bibcite{wang2018video}{{49}{}{{}}{{}}}
\bibcite{vedantam2015cider}{{50}{}{{}}{{}}}
\bibcite{banerjee2005meteor}{{51}{}{{}}{{}}}
\bibcite{papineni2002bleu}{{52}{}{{}}{{}}}
\bibcite{you2016image}{{53}{}{{}}{{}}}
\bibcite{wu2018hierarchical}{{54}{}{{}}{{}}}
\bibstyle{ieeetr}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
